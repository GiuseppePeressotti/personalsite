[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bio",
    "section": "",
    "text": "Welcome! I am Giuseppe Peressotti, 1st year PhD student in Public Policy and Political Economy from The University of Texas at Dallas(EPPS).\nMy main field of study would be International Trade, with a focus on Preferential Trade Agreements. Some of my specific research topics include Digital Provisions in PTAs, the relationship between foreign policy and trade interconnection between states, and the influence of trade ties on the preferential procurement of goods.\nKeep following me for more updates on further work and publications (soon to come).\nYou can find my CV here (WORK IN PROGRESS)."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Digital Provision Clustering in Trade Policy: Are Economic Factors Drivers of Digital Regionalism?\nDelving into the Concept of Trade Diplomacy: A Dual Comparative Case Study on the US and China\nVaccine Diplomacy: The Role of US Bilateral Relations in COVID Vaccine Procurements (joint paper)"
  },
  {
    "objectID": "research.html#work-in-progress",
    "href": "research.html#work-in-progress",
    "title": "Research",
    "section": "",
    "text": "Digital Provision Clustering in Trade Policy: Are Economic Factors Drivers of Digital Regionalism?\nDelving into the Concept of Trade Diplomacy: A Dual Comparative Case Study on the US and China\nVaccine Diplomacy: The Role of US Bilateral Relations in COVID Vaccine Procurements (joint paper)"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Syllabus available here"
  },
  {
    "objectID": "teaching.html#epps2301---research-design-in-the-social-and-policy-sciences",
    "href": "teaching.html#epps2301---research-design-in-the-social-and-policy-sciences",
    "title": "Teaching",
    "section": "",
    "text": "Syllabus available here"
  },
  {
    "objectID": "assign01.html",
    "href": "assign01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "About the Site\nThis site is powered by Quarto and will display my academic profile, including my bio, research, teaching, and contact information. While it is going to update for the current class, I will make further modifications in order to achieve a professional site by the time I will be on the job market (Fall 2026).\nI utilize “Lux” as the site theme, which can be customized in the _quarto.yml file. The theme is responsive and adapts to different screen sizes, ensuring a consistent (yet professional) look independent of the device used to access it. The site is structured with a navigation bar at the top, allowing easy access to different sections such as “Home”, “Research”, “Teaching” and “Assignments”. My CV can also be downloaded directly from the Home page (not updated)."
  },
  {
    "objectID": "assign02.html",
    "href": "assign02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Analysis by Downloading Google Trends Data\n\n# analyze the data after cleaning it\nelection &lt;- read.csv('/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_2/election.csv', skip = 1)\nnames(election) &lt;- c(\"date\", \"hits\")\nelection$hits &lt;- as.character(election$hits)\nelection$hits &lt;- ifelse(election$hits == \"&lt;1\", 0, as.numeric(election$hits))\n\nWarning in ifelse(election$hits == \"&lt;1\", 0, as.numeric(election$hits)): NAs\nintroduced by coercion\n\nggplot(election, aes(x = date, y = hits, group = 1)) +\n  geom_line()\n\n\n\n\n\n\n\n# kamala data\n\nkamala &lt;- read.csv('/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_2/kamala.csv', skip = 1)\nnames(kamala) &lt;- c(\"date\", \"hits\")\nkamala$hits &lt;- as.character(kamala$hits)\nkamala$hits &lt;- ifelse(kamala$hits == \"&lt;1\", 0, as.numeric(kamala$hits))\n\nWarning in ifelse(kamala$hits == \"&lt;1\", 0, as.numeric(kamala$hits)): NAs\nintroduced by coercion\n\nggplot(kamala, aes(x = date, y = hits, group = 1)) +\n  geom_line()\n\n\n\n\n\n\n\n# trump data \n\ntrump &lt;- read.csv('/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_2/trump.csv', skip = 1)\nnames(trump) &lt;- c(\"date\", \"hits\")\ntrump$hits &lt;- as.character(trump$hits)\ntrump$hits &lt;- ifelse(trump$hits == \"&lt;1\", 0, as.numeric(trump$hits))\n\nggplot(trump, aes(x = date, y = hits, group = 1)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThe above code relies on the 3 different CSV files downloaded from Google Trends. The data is not clean and not properly formatted to be analyzed in R. As such, further data cleaning was necessary to visualize the data. Based on the graphs, we can see the trends for each term. For election, it is clear that American election seem to dominate searches with a seasonality of 4 years. Trump and Kamala present similar matches in terms of search trends, with spikes preceding elections in 2016, 2020 and 2024. Trump has higher search interest, especially for the 2016 and 2020 elections, showcasing his importance as a political candidate.\n\n\nAnalysis by gtrendsR\n\n# analyze with gtrendsR\n\n# class code -----\n\n## EPPS 6302 Methods of Data Collection and Production\n## Google Trends with R\n\nTrumpHarrisElection = gtrends(c(\"Trump\",\"Harris\",\"election\"), onlyInterest = TRUE, geo = \"US\", gprop = \"web\", time = \"today+5-y\", category = 0, ) # last five years\nthe_df=TrumpHarrisElection$interest_over_time\nplot(TrumpHarrisElection)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the gtrendsR package.\n  Please report the issue at &lt;https://github.com/PMassicotte/gtrendsR/issues&gt;.\n\n\n\n\n\n\n\n\ntg = gtrends(\"tariff\", time = \"all\")\n\nUtilizing the gtrendsR package yields similar results as the manual download from google trends. Differences stand in how streamlined the process is, and the data being formatted in an R-friendly way. Substantive results are identical."
  },
  {
    "objectID": "assign03.html",
    "href": "assign03.html",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "library(tidycensus)\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(readr)\n\n#API ALREADY SAVED AND CENSORED\n\n\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n\ntx_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_inc = \"B19013_001\",\n    poverty = \"B17001_002\"\n  ),\n  state = \"TX\",\n  year = 2023,\n  geometry = TRUE,\n  output = \"wide\"\n)\n\nGetting data from the 2019-2023 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\nggplot(tx_data) +\n  geom_sf(aes(fill = median_incE), color = NA) +\n  scale_fill_viridis_c(option = \"plasma\", na.value = \"grey90\") +\n  labs(\n    title = \"Median Household Income (Texas Counties, 2023 ACS)\",\n    fill = \"Income ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\ntop10_poverty &lt;- tx_data %&gt;%\n  arrange(desc(povertyE)) %&gt;%\n  slice(1:10) %&gt;%\n  select(NAME, povertyE, povertyM)\n\nbottom10_poverty &lt;- tx_data %&gt;%\n  arrange(povertyE) %&gt;%\n  slice(1:10) %&gt;%\n  select(NAME, povertyE, povertyM)\n\ntop10_poverty\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 25.83738 xmax: -94.9085 ymax: 33.43045\nGeodetic CRS:  NAD83\n                    NAME povertyE povertyM                       geometry\n1   Harris County, Texas   749481    15891 MULTIPOLYGON (((-94.97839 2...\n2   Dallas County, Texas   359950    10475 MULTIPOLYGON (((-97.03852 3...\n3    Bexar County, Texas   294002     8323 MULTIPOLYGON (((-98.80655 2...\n4  Hidalgo County, Texas   237121     8739 MULTIPOLYGON (((-98.58634 2...\n5  Tarrant County, Texas   229884     7849 MULTIPOLYGON (((-97.55053 3...\n6  El Paso County, Texas   160998     7148 MULTIPOLYGON (((-106.6455 3...\n7   Travis County, Texas   140926     6636 MULTIPOLYGON (((-98.15927 3...\n8  Cameron County, Texas   102583     4345 MULTIPOLYGON (((-97.24047 2...\n9   Collin County, Texas    69846     4614 MULTIPOLYGON (((-96.8441 32...\n10  Denton County, Texas    65649     4233 MULTIPOLYGON (((-97.39826 3...\n\nbottom10_poverty\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -103.9839 ymin: 26.598 xmax: -97.29015 ymax: 36.05771\nGeodetic CRS:  NAD83\n                      NAME povertyE povertyM                       geometry\n1     Kenedy County, Texas        3        5 MULTIPOLYGON (((-97.39839 2...\n2     Loving County, Texas        5        7 MULTIPOLYGON (((-103.9839 3...\n3       King County, Texas       29       26 MULTIPOLYGON (((-100.5187 3...\n4     Borden County, Texas       35       28 MULTIPOLYGON (((-101.6913 3...\n5   Sterling County, Texas       37       25 MULTIPOLYGON (((-101.267 31...\n6    Roberts County, Texas       50       35 MULTIPOLYGON (((-101.086 35...\n7       Kent County, Texas       56       32 MULTIPOLYGON (((-101.0389 3...\n8   McMullen County, Texas       57       47 MULTIPOLYGON (((-98.80251 2...\n9    Terrell County, Texas       70       46 MULTIPOLYGON (((-102.5669 3...\n10 Glasscock County, Texas       91       67 MULTIPOLYGON (((-101.7761 3...\n\n\n\nInterpretation\nThe map that I created visualizes median household income across Texas using 2023 census data. Based on the variation of the gradient, we can identify the richest counties that are mostly clustered around metropolititan areas, with the deep purple color identifying the more remote regions on the far West area of Texas. The tables identify the areas with the highest (and bottom) families in terms of poverty (below poverty line): the counties with the largest absolute number of residents in poverty are: Harris, Dallas, Bexar, Hidalgo, Tarrant, El Paso, and Travis. Those are followed by border suburban counties like Cameron, Collin, and Denton. These counties appear at the top not necessarily because they are the poorest, but because they are large-population counties."
  },
  {
    "objectID": "assign04.html",
    "href": "assign04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(stringr)\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n# Wikipedia URL for population\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\n\n# Read the HTML content from the page\nwiki_pop &lt;- read_html(url)\nclass(wiki_pop)  # should return \"xml_document\" \"xml_node\"\n\n[1] \"xml_document\" \"xml_node\"    \n\n# Use XPath to select the main population table\npop_table &lt;- wiki_pop %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table()\n\n# Convert to data frame\npop_df &lt;- pop_table[[1]]\n\n# Rename columns to match the Wikipedia table\ncolnames(pop_df) &lt;- c(\"Location\", \"Population\", \"PercentWorld\", \"Date\", \"Source\", \"Notes\")\n\n# Remove trailing notes or citations in Location and Date\npop_df$Location &lt;- str_split_fixed(pop_df$Location, \"\\\\[\", n = 2)[,1] %&gt;% trimws()\npop_df$Date &lt;- str_split_fixed(pop_df$Date, \"\\\\[\", n = 2)[,1] %&gt;% trimws()\n\n# Convert Population to numeric (remove commas)\npop_df$Population &lt;- as.numeric(gsub(\",\", \"\", pop_df$Population))\n\n# Convert PercentWorld to numeric (remove % sign)\npop_df$PercentWorld &lt;- as.numeric(gsub(\"%\", \"\", pop_df$PercentWorld))\n\n# Inspect first 10 rows\nhead(pop_df, 10)\n\n# A tibble: 10 × 6\n   Location      Population PercentWorld Date        Source                Notes\n   &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;                 &lt;chr&gt;\n 1 World         8232000000        100   13 Jun 2025 UN projection[1][3]   \"\"   \n 2 India         1417492000         17.3 1 Jul 2025  Official projection[… \"[b]\"\n 3 China         1408280000         17.1 31 Dec 2024 Official estimate[5]  \"[c]\"\n 4 United States  340110988          4.1 1 Jul 2024  Official estimate[6]  \"[d]\"\n 5 Indonesia      284438782          3.5 30 Jun 2025 National annual proj… \"\"   \n 6 Pakistan       241499431          2.9 1 Mar 2023  2023 census result[8] \"[e]\"\n 7 Nigeria        223800000          2.7 1 Jul 2023  Official projection[… \"\"   \n 8 Brazil         213421037          2.6 1 Jul 2025  Official estimate[10] \"\"   \n 9 Bangladesh     169828911          2.1 14 Jun 2022 2022 census result[1… \"[f]\"\n10 Russia         146028325          1.8 1 Jan 2025  Official estimate[13] \"[g]\"\n\n\n\nDiscussion\nI adapted the code to scrape the wikipedia page for population (by country). With similar code we can gain access to any sort of html-based table and obtain data that is easily convertible to data frames. The code uses the rvest package to read the HTML content of the page, then uses XPath to select the specific table containing population data. After extracting the table, I cleaned the data by removing any trailing notes or citations from the Location and Date columns, and converted the Population and PercentWorld columns to numeric types for easier analysis. Finally, I displayed the first 10 rows of the cleaned data frame to verify that the scraping and cleaning process was successful."
  },
  {
    "objectID": "assign05.html",
    "href": "assign05.html",
    "title": "Assignment 5",
    "section": "",
    "text": "## Scraping “Foreign Relations Committee” documents from GovInfo\n\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:jsonlite':\n\n    flatten\n\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\napi_key &lt;- \"4Extd9hhomZZ85LFaZec5xuYWuQSlxIzbTH8IjFc\"\n\n# ------------------------------\n# 1. API SEARCH REQUEST\n# ------------------------------\n\nbody &lt;- list(\n  query = 'Trade AND \"Foreign Relations Committee\"',\n  pageSize = 10,         \n  offsetMark = \"*\"\n)\n\nresp &lt;- POST(\n  url   = \"https://api.govinfo.gov/search\",\n  query = list(api_key = api_key),\n  encode = \"json\",\n  body = body\n)\n\nres &lt;- jsonlite::fromJSON(content(resp, \"text\", encoding = \"UTF-8\"))\n\ngovfiles &lt;- res$results\n\n# ------------------------------\n# 2. Construct PDF Links\n# ------------------------------\n\ngovfiles$index &lt;- seq_len(nrow(govfiles))\ngovfiles$pdf_url &lt;- paste0(\n  \"https://www.govinfo.gov/content/pkg/\",\n  govfiles$packageId,\n  \"/pdf/\",\n  govfiles$packageId,\n  \".pdf\"\n)\n\n# ------------------------------\n# 3. Prepare for Downloading\n# ------------------------------\n\nsave_dir &lt;- '/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5'\ndir.create(save_dir, recursive = TRUE, showWarnings = FALSE)\n\n# ------------------------------\n# 4. Download Function\n# ------------------------------\n\ndownload_one &lt;- function(url, id) {\n  \n  destfile &lt;- file.path(save_dir, paste0(\"govfile_\", id, \".pdf\"))\n  \n  tryCatch({\n    download.file(url, destfile, mode = \"wb\")\n    Sys.sleep(runif(1, 1.5, 3))   # cooldown\n    message(\"Downloaded: \", destfile)\n  }, error = function(e) {\n    message(\"FAILED: \", url)\n  })\n}\n\n# ------------------------------\n# 5. Download the 10 PDFs\n# ------------------------------\n\nwalk2(govfiles$pdf_url, govfiles$index, download_one)\n\nDownloaded: /Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5/govfile_1.pdf\n\n\nDownloaded: /Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5/govfile_2.pdf\n\n\nDownloaded: /Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5/govfile_3.pdf\n\n\nDownloaded: /Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5/govfile_4.pdf\n\n\nDownloaded: /Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5/govfile_5.pdf\n\n\nDownloaded: /Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5/govfile_6.pdf\n\n\nDownloaded: /Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5/govfile_7.pdf\n\n\nDownloaded: /Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5/govfile_8.pdf\n\n\nDownloaded: /Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5/govfile_9.pdf\n\n\nDownloaded: /Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5/govfile_10.pdf\n\n\n\nReport\nThe scraping process using the GovInfo API worked well overall. After implementing a few modifications to the original script, which comprised mainly refining the POST query structure, adjusting the search parameters, and constructing reliable PDF download links, the workflow successfully retrieved and downloaded the desired government documents related to the Foreign Relations Committee. Scraping functioned smoothly once the query had been properly structured.\nHowever, several practical issues emerged during testing. First, the quality and usefulness of the results depend heavily on the chosen search terms (playing around with different number of documents). Minor variations in phrasing or keyword placement can return a different set of documents, some of which may be irrelevant, incomplete, or even entirely void. Additionally, depending on the size and number of files requested, the downloading process may take longer, and the availability of PDF versions is not guaranteed for all entries.\nThe overall scraped corpus is still valuable. While the PDFs tend to be noisy—containing artifacts from OCR processing, embedded images, or poorly structured text—they nevertheless offer a workable dataset for text analysis and natural language processing. With appropriate preprocessing, they can be transformed into a rich source for extracting themes, patterns, or policy-relevant language.\nRefining the search keywords, imposing more precise constraints (such as specific committees, date ranges, or document types), and filtering results programmatically before downloading can increase the quality and relevance of the final dataset."
  },
  {
    "objectID": "assign06.html#title-assignment-6",
    "href": "assign06.html#title-assignment-6",
    "title": "Comment",
    "section": "title: “Assignment 6”",
    "text": "title: “Assignment 6”\n\n##############################################################\n\nlibrary(quanteda)\n\nPackage version: 4.3.1\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(quanteda.textstats)\n\n\n# ------------------------------------------------------------\n# 1. Load the data\n# ------------------------------------------------------------\n\nsummit &lt;- read_csv(\n  \"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\"\n)\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# ------------------------------------------------------------\n# 2. Improved preprocessing\n# ------------------------------------------------------------\n\nclean_tweet_text &lt;- function(x) {\n  x %&gt;%\n    str_replace_all(\"http[^\\\\s]+\", \"\") %&gt;%         # URLs\n    str_replace_all(\"RT\\\\s+@\\\\w+:\", \"\") %&gt;%        # RT flag\n    str_replace_all(\"@\\\\w+\", \"\") %&gt;%               # remove @mentions (kept separately later)\n    str_replace_all(\"#\", \"\") %&gt;%                   # remove hash marks\n    str_replace_all(\"[[:punct:]]+\", \" \") %&gt;%       # punctuation\n    str_squish() %&gt;%                               # collapse whitespace\n    tolower()\n}\n\nsummit &lt;- summit %&gt;%\n  mutate(\n    text_clean = clean_tweet_text(text),\n    text_nohash = str_replace_all(text, \"#\", \"\")\n  )\n\n# ------------------------------------------------------------\n# 3. Tokenization and dfm creation\n# ------------------------------------------------------------\n\ntoks &lt;- tokens(\n  summit$text_clean,\n  remove_punct = TRUE,\n  remove_numbers = TRUE\n) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_remove(c(\"rt\", \"amp\")) # common Twitter junk tokens\n\ndfm_sum &lt;- dfm(toks) %&gt;%\n  dfm_trim(min_termfreq = 5)\n\n# Top terms\ntop_terms &lt;- topfeatures(dfm_sum, 30)\nprint(top_terms)\n\n    biden        xi    summit   virtual         s     china president   jinping \n    15613     14263     11229      7491      7415      6369      6168      4878 \n  chinese        us    monday       joe      hold    taiwan         u  tensions \n     2762      2733      2625      2612      2301      1827      1545      1421 \n      old    friend      said     covid  expected   meeting     white     house \n     1246      1196      1113      1043      1034       929       927       924 \n        t  conflict     calls      told    leader     ahead \n      887       868       847       797       782       781 \n\n# ------------------------------------------------------------\n# 4. LSA on TF–IDF weighted dfm\n# ------------------------------------------------------------\n\ndfm_tfidf &lt;- dfm_tfidf(dfm_sum)\nlsa_model &lt;- textmodel_lsa(dfm_tfidf, nd = 4)\nsummary(lsa_model)\n\n                Length   Class     Mode   \nsk                     4 -none-    numeric\ndocs               58080 -none-    numeric\nfeatures           12592 -none-    numeric\nmatrix_low_rank 45708960 -none-    numeric\ndata            45708960 dgCMatrix S4     \n\n# Document coordinates\ndoc_coords &lt;- as.data.frame(lsa_model$docs) %&gt;%\n  rownames_to_column(\"doc_id\") %&gt;%\n  mutate(doc_id = as.numeric(doc_id))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `doc_id = as.numeric(doc_id)`.\nCaused by warning:\n! NAs introduced by coercion\n\n# Add metadata (optional)\nif (\"user\" %in% names(summit)) {\n  doc_coords &lt;- doc_coords %&gt;%\n    left_join(\n      summit %&gt;% mutate(doc_id = row_number()) %&gt;% select(doc_id, user),\n      by = \"doc_id\"\n    )\n}\n\n# ------------------------------------------------------------\n# 5. Hashtag analysis (network)\n# ------------------------------------------------------------\n\nhashtags &lt;- tokens(\n  summit$text,\n  remove_punct = FALSE\n) %&gt;%\n  tokens_select(\"#*\", selection = \"keep\") %&gt;%\n  dfm()\n\ntop_hashtags &lt;- topfeatures(hashtags, 30)\nprint(top_hashtags)\n\n         #china          #biden      #xijinping       #joebiden        #america \n            790             691             640             447             308 \n     #americans    #coronavirus       #fentanyl             #xi             #us \n            296             295             295             225             160 \n#uyghurgenocide         #taiwan        #foxnews            #usa       #breaking \n            152             149             144             111             103 \n          #news            #ccp    #humanrights        #uyghurs       #tibetans \n             86              84              77              73              67 \n #bidenxisummit         #summit       #hongkong        #updates        #covid19 \n             50              46              43              40              38 \n      #xinjiang       #politics               #      #exclusive     #technology \n             37              36              33              29              29 \n\ntag_fcm &lt;- fcm(hashtags)\ntop_tagnames &lt;- names(top_hashtags)\n\ntag_fcm_sel &lt;- fcm_select(tag_fcm, pattern = top_tagnames)\n\ntextplot_network(\n  tag_fcm_sel,\n  min_freq = 20,\n  edge_alpha = 0.8,\n  edge_size = 1\n)\n\n\n\n\n\n\n\n# ------------------------------------------------------------\n# 6. User mention network (@users)\n# ------------------------------------------------------------\n\nuser_tokens &lt;- tokens(\n  summit$text,\n  remove_punct = FALSE\n) %&gt;%\n  tokens_select(\"@*\", selection = \"keep\")\n\nuser_dfm &lt;- dfm(user_tokens)\ntop_users &lt;- topfeatures(user_dfm, 30)\n\nuser_fcm &lt;- fcm(user_dfm)\nuser_fcm_sel &lt;- fcm_select(user_fcm, names(top_users))\n\ntextplot_network(\n  user_fcm_sel,\n  min_freq = 10,\n  edge_alpha = 0.8\n)\n\n\n\n\n\n\n\n# ------------------------------------------------------------\n# 7. Save output files\n# ------------------------------------------------------------\n\nsave_path &lt;- \"/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_6/\"\n\nwrite_rds(dfm_sum, paste0(save_path, \"dfm_summit.rds\"))\nwrite_csv(doc_coords, paste0(save_path, \"lsa_document_coords.csv\"))\n\nmessage(\"Files saved successfully to:\")\n\nFiles saved successfully to:\n\nmessage(save_path)\n\n/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_6/\n\n##############################################################\n\n\n# Analyzing 3 different eras\n\n\n\n# -------------------------\n# 1. Define historical eras\n# -------------------------\n\ninaug &lt;- data_corpus_inaugural\n\ninaug$Era &lt;- case_when(\n  inaug$Year &lt;= 1865 ~ \"Early Republic\",\n  inaug$Year &gt; 1865 & inaug$Year &lt;= 1945 ~ \"Industrial/Progressive\",\n  inaug$Year &gt; 1945 & inaug$Year &lt;= 1991 ~ \"Cold War\",\n  inaug$Year &gt; 1991 ~ \"Modern\"\n)\n\n# -------------------------\n# 2. Construct DFM by Era\n# -------------------------\n\ndfm_era &lt;- tokens(inaug, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = inaug$Era) %&gt;%\n  dfm_trim(min_termfreq = 10)\n\n# -------------------------\n# 3. Compare eras using keyness\n#    Example: Modern vs. Early Republic\n# -------------------------\n\nkey_era &lt;- textstat_keyness(dfm_era, target = \"Modern\")\n\ntextplot_keyness(key_era, n = 20)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the quanteda.textplots package.\n  Please report the issue at\n  &lt;https://github.com/quanteda/quanteda.textplots/issues&gt;.\n\n\n\n\n\n\n\n\n# -------------------------\n# 4. Compare frequency of ideological terms over eras\n# -------------------------\n\nfocus_terms &lt;- c(\"freedom\", \"war\", \"economy\", \"people\", \"nation\")\n\ndfm_rel &lt;- dfm_weight(dfm_era, scheme = \"prop\") * 100  \n\n# Add Era docvar\ndocvars(dfm_rel, \"Era\") &lt;- docnames(dfm_rel)\n\nfreq_rel &lt;- textstat_frequency(dfm_rel, groups = docvars(dfm_rel, \"Era\"))\nfreq_focus &lt;- subset(freq_rel, feature %in% focus_terms)\n\n\nfreq_focus &lt;- subset(freq_rel, feature %in% focus_terms)\n\nggplot(freq_focus, aes(x = group, y = frequency, color = feature)) +\n  geom_point(size = 3) +\n  geom_line(aes(group = feature), linewidth = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Change in Rhetorical Themes Across Historical Eras\",\n    x = \"Era\",\n    y = \"Relative Frequency (%)\",\n    color = \"Term\"\n  )\n\n\n\n\n\n\n\n# -------------------------\n# 5. Top words in each era (comparison wordcloud)\n# -------------------------\n\ntextplot_wordcloud(dfm_era, comparison = TRUE, max_words = 100)\n\n\n\n\n\n\n\n# -------------------------\n# 6. Measure stylistic / ideological shift using Wordfish\n# -------------------------\n\nwf &lt;- textmodel_wordfish(dfm(tokens(inaug)), dir = c(1, 10))\n\nwf_df &lt;- data.frame(\n  Year = inaug$Year,\n  Era  = inaug$Era,\n  Position = wf$docs\n)\n\nggplot(wf_df, aes(x = Year, y = Position, color = Era)) +\n  geom_line(size = 1.1) +\n  geom_point(size = 2) +\n  theme_minimal() +\n  labs(\n    title = \"Wordfish Ideological/Rhetorical Position Over Time\",\n    y = \"Estimated Wordfish Position\"\n  )\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?"
  },
  {
    "objectID": "assign06.html",
    "href": "assign06.html",
    "title": "Assignment 6",
    "section": "",
    "text": "##############################################################\n\nlibrary(quanteda)\n\nPackage version: 4.3.1\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(quanteda.textstats)\n\n\n# ------------------------------------------------------------\n# 1. Load the data\n# ------------------------------------------------------------\n\nsummit &lt;- read_csv(\n  \"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\"\n)\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# ------------------------------------------------------------\n# 2. Improved preprocessing\n# ------------------------------------------------------------\n\nclean_tweet_text &lt;- function(x) {\n  x %&gt;%\n    str_replace_all(\"http[^\\\\s]+\", \"\") %&gt;%         # URLs\n    str_replace_all(\"RT\\\\s+@\\\\w+:\", \"\") %&gt;%        # RT flag\n    str_replace_all(\"@\\\\w+\", \"\") %&gt;%               # remove @mentions (kept separately later)\n    str_replace_all(\"#\", \"\") %&gt;%                   # remove hash marks\n    str_replace_all(\"[[:punct:]]+\", \" \") %&gt;%       # punctuation\n    str_squish() %&gt;%                               # collapse whitespace\n    tolower()\n}\n\nsummit &lt;- summit %&gt;%\n  mutate(\n    text_clean = clean_tweet_text(text),\n    text_nohash = str_replace_all(text, \"#\", \"\")\n  )\n\n# ------------------------------------------------------------\n# 3. Tokenization and dfm creation\n# ------------------------------------------------------------\n\ntoks &lt;- tokens(\n  summit$text_clean,\n  remove_punct = TRUE,\n  remove_numbers = TRUE\n) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_remove(c(\"rt\", \"amp\")) # common Twitter junk tokens\n\ndfm_sum &lt;- dfm(toks) %&gt;%\n  dfm_trim(min_termfreq = 5)\n\n# Top terms\ntop_terms &lt;- topfeatures(dfm_sum, 30)\nprint(top_terms)\n\n    biden        xi    summit   virtual         s     china president   jinping \n    15613     14263     11229      7491      7415      6369      6168      4878 \n  chinese        us    monday       joe      hold    taiwan         u  tensions \n     2762      2733      2625      2612      2301      1827      1545      1421 \n      old    friend      said     covid  expected   meeting     white     house \n     1246      1196      1113      1043      1034       929       927       924 \n        t  conflict     calls      told    leader     ahead \n      887       868       847       797       782       781 \n\n# ------------------------------------------------------------\n# 4. LSA on TF–IDF weighted dfm\n# ------------------------------------------------------------\n\ndfm_tfidf &lt;- dfm_tfidf(dfm_sum)\nlsa_model &lt;- textmodel_lsa(dfm_tfidf, nd = 4)\nsummary(lsa_model)\n\n                Length   Class     Mode   \nsk                     4 -none-    numeric\ndocs               58080 -none-    numeric\nfeatures           12592 -none-    numeric\nmatrix_low_rank 45708960 -none-    numeric\ndata            45708960 dgCMatrix S4     \n\n# Document coordinates\ndoc_coords &lt;- as.data.frame(lsa_model$docs) %&gt;%\n  rownames_to_column(\"doc_id\") %&gt;%\n  mutate(doc_id = as.numeric(doc_id))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `doc_id = as.numeric(doc_id)`.\nCaused by warning:\n! NAs introduced by coercion\n\n# Add metadata (optional)\nif (\"user\" %in% names(summit)) {\n  doc_coords &lt;- doc_coords %&gt;%\n    left_join(\n      summit %&gt;% mutate(doc_id = row_number()) %&gt;% select(doc_id, user),\n      by = \"doc_id\"\n    )\n}\n\n# ------------------------------------------------------------\n# 5. Hashtag analysis (network)\n# ------------------------------------------------------------\n\nhashtags &lt;- tokens(\n  summit$text,\n  remove_punct = FALSE\n) %&gt;%\n  tokens_select(\"#*\", selection = \"keep\") %&gt;%\n  dfm()\n\ntop_hashtags &lt;- topfeatures(hashtags, 30)\nprint(top_hashtags)\n\n         #china          #biden      #xijinping       #joebiden        #america \n            790             691             640             447             308 \n     #americans    #coronavirus       #fentanyl             #xi             #us \n            296             295             295             225             160 \n#uyghurgenocide         #taiwan        #foxnews            #usa       #breaking \n            152             149             144             111             103 \n          #news            #ccp    #humanrights        #uyghurs       #tibetans \n             86              84              77              73              67 \n #bidenxisummit         #summit       #hongkong        #updates        #covid19 \n             50              46              43              40              38 \n      #xinjiang       #politics               #      #exclusive     #technology \n             37              36              33              29              29 \n\ntag_fcm &lt;- fcm(hashtags)\ntop_tagnames &lt;- names(top_hashtags)\n\ntag_fcm_sel &lt;- fcm_select(tag_fcm, pattern = top_tagnames)\n\ntextplot_network(\n  tag_fcm_sel,\n  min_freq = 20,\n  edge_alpha = 0.8,\n  edge_size = 1\n)\n\n\n\n\n\n\n\n# ------------------------------------------------------------\n# 6. User mention network (@users)\n# ------------------------------------------------------------\n\nuser_tokens &lt;- tokens(\n  summit$text,\n  remove_punct = FALSE\n) %&gt;%\n  tokens_select(\"@*\", selection = \"keep\")\n\nuser_dfm &lt;- dfm(user_tokens)\ntop_users &lt;- topfeatures(user_dfm, 30)\n\nuser_fcm &lt;- fcm(user_dfm)\nuser_fcm_sel &lt;- fcm_select(user_fcm, names(top_users))\n\ntextplot_network(\n  user_fcm_sel,\n  min_freq = 10,\n  edge_alpha = 0.8\n)\n\n\n\n\n\n\n\n# ------------------------------------------------------------\n# 7. Save output files\n# ------------------------------------------------------------\n\nsave_path &lt;- \"/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_6/\"\n\nwrite_rds(dfm_sum, paste0(save_path, \"dfm_summit.rds\"))\nwrite_csv(doc_coords, paste0(save_path, \"lsa_document_coords.csv\"))\n\nmessage(\"Files saved successfully to:\")\n\nFiles saved successfully to:\n\nmessage(save_path)\n\n/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_6/\n\n##############################################################\n\n\n# Analyzing 3 different eras\n\n\n\n# -------------------------\n# 1. Define historical eras\n# -------------------------\n\ninaug &lt;- data_corpus_inaugural\n\ninaug$Era &lt;- case_when(\n  inaug$Year &lt;= 1865 ~ \"Early Republic\",\n  inaug$Year &gt; 1865 & inaug$Year &lt;= 1945 ~ \"Industrial/Progressive\",\n  inaug$Year &gt; 1945 & inaug$Year &lt;= 1991 ~ \"Cold War\",\n  inaug$Year &gt; 1991 ~ \"Modern\"\n)\n\n# -------------------------\n# 2. Construct DFM by Era\n# -------------------------\n\ndfm_era &lt;- tokens(inaug, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = inaug$Era) %&gt;%\n  dfm_trim(min_termfreq = 10)\n\n# -------------------------\n# 3. Compare eras using keyness\n#    Example: Modern vs. Early Republic\n# -------------------------\n\nkey_era &lt;- textstat_keyness(dfm_era, target = \"Modern\")\n\ntextplot_keyness(key_era, n = 20)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the quanteda.textplots package.\n  Please report the issue at\n  &lt;https://github.com/quanteda/quanteda.textplots/issues&gt;.\n\n\n\n\n\n\n\n\n# -------------------------\n# 4. Compare frequency of ideological terms over eras\n# -------------------------\n\nfocus_terms &lt;- c(\"freedom\", \"war\", \"economy\", \"people\", \"nation\")\n\ndfm_rel &lt;- dfm_weight(dfm_era, scheme = \"prop\") * 100  \n\n# Add Era docvar\ndocvars(dfm_rel, \"Era\") &lt;- docnames(dfm_rel)\n\nfreq_rel &lt;- textstat_frequency(dfm_rel, groups = docvars(dfm_rel, \"Era\"))\nfreq_focus &lt;- subset(freq_rel, feature %in% focus_terms)\n\n\nfreq_focus &lt;- subset(freq_rel, feature %in% focus_terms)\n\nggplot(freq_focus, aes(x = group, y = frequency, color = feature)) +\n  geom_point(size = 3) +\n  geom_line(aes(group = feature), linewidth = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Change in Rhetorical Themes Across Historical Eras\",\n    x = \"Era\",\n    y = \"Relative Frequency (%)\",\n    color = \"Term\"\n  )\n\n\n\n\n\n\n\n# -------------------------\n# 5. Top words in each era (comparison wordcloud)\n# -------------------------\n\ntextplot_wordcloud(dfm_era, comparison = TRUE, max_words = 100)\n\n\n\n\n\n\n\n# -------------------------\n# 6. Measure stylistic / ideological shift using Wordfish\n# -------------------------\n\nwf &lt;- textmodel_wordfish(dfm(tokens(inaug)), dir = c(1, 10))\n\nwf_df &lt;- data.frame(\n  Year = inaug$Year,\n  Era  = inaug$Era,\n  Position = wf$docs\n)\n\nggplot(wf_df, aes(x = Year, y = Position, color = Era)) +\n  geom_line(size = 1.1) +\n  geom_point(size = 2) +\n  theme_minimal() +\n  labs(\n    title = \"Wordfish Ideological/Rhetorical Position Over Time\",\n    y = \"Estimated Wordfish Position\"\n  )\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\n\nComment\nLooking at U.S. inaugural speeches over the years, there’s a clear shift in what presidents focus on. In the early eras, like the Early Republic and Industrial/Progressive periods, speeches tended to emphasize big-picture ideas like government, power, peace, and war. The tone was more about the nation’s position in the world and the workings of the state.\nOver time, especially from the Cold War onward, the focus moves more toward domestic issues. Words like America, American, people, and economy show up a lot more. References to war or international conflict become less common, while talking about the country’s citizens and their well-being takes center stage. This is mostly reflected on the second graph."
  }
]