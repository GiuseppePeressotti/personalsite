---
title: "Assignment 4"

---

```{r}
library(tidyverse)
library(stringr)
library(rvest)


# Wikipedia URL for population
url <- 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'

# Read the HTML content from the page
wiki_pop <- read_html(url)
class(wiki_pop)  # should return "xml_document" "xml_node"

# Use XPath to select the main population table
pop_table <- wiki_pop %>%
  html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[1]') %>%
  html_table()

# Convert to data frame
pop_df <- pop_table[[1]]

# Rename columns to match the Wikipedia table
colnames(pop_df) <- c("Location", "Population", "PercentWorld", "Date", "Source", "Notes")

# Remove trailing notes or citations in Location and Date
pop_df$Location <- str_split_fixed(pop_df$Location, "\\[", n = 2)[,1] %>% trimws()
pop_df$Date <- str_split_fixed(pop_df$Date, "\\[", n = 2)[,1] %>% trimws()

# Convert Population to numeric (remove commas)
pop_df$Population <- as.numeric(gsub(",", "", pop_df$Population))

# Convert PercentWorld to numeric (remove % sign)
pop_df$PercentWorld <- as.numeric(gsub("%", "", pop_df$PercentWorld))

# Inspect first 10 rows
head(pop_df, 10)
```

# Discussion

I adapted the code to scrape the wikipedia page for population (by country). With similar code we can gain access to any sort of html-based table and obtain data that is easily convertible to data frames. The code uses the rvest package to read the HTML content of the page, then uses XPath to select the specific table containing population data. After extracting the table, I cleaned the data by removing any trailing notes or citations from the Location and Date columns, and converted the Population and PercentWorld columns to numeric types for easier analysis. Finally, I displayed the first 10 rows of the cleaned data frame to verify that the scraping and cleaning process was successful.