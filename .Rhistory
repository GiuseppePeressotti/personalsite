# ------------------------------
body <- list(
query = 'Trade AND "Foreign Relations Committee"',
pageSize = 10,          # <-- ONLY 10 MOST RECENT
offsetMark = "*"
)
resp <- POST(
url   = "https://api.govinfo.gov/search",
query = list(api_key = api_key),
encode = "json",
body = body
)
## Scraping “Foreign Relations Committee” documents from GovInfo
library(httr)
library(jsonlite)
library(dplyr)
library(purrr)
library(magrittr)
api_key <- "4Extd9hhomZZ85LFaZec5xuYWuQSlxIzbTH8IjFc"
# ------------------------------
# 1. API SEARCH REQUEST
# ------------------------------
body <- list(
query = 'Trade AND "Foreign Relations Committee"',
pageSize = 10,          # <-- ONLY 10 MOST RECENT
offsetMark = "*"
)
resp <- POST(
url   = "https://api.govinfo.gov/search",
query = list(api_key = api_key),
encode = "json",
body = body
)
res <- jsonlite::fromJSON(content(resp, "text", encoding = "UTF-8"))
govfiles <- res$results
# ------------------------------
# 2. Construct PDF Links
# ------------------------------
govfiles$index <- seq_len(nrow(govfiles))
govfiles$pdf_url <- paste0(
"https://www.govinfo.gov/content/pkg/",
govfiles$packageId,
"/pdf/",
govfiles$packageId,
".pdf"
)
# ------------------------------
# 3. Prepare for Downloading
# ------------------------------
save_dir <- "C:/Users/chank/Downloads/chan_gov_files/pdfs"
dir.create(save_dir, recursive = TRUE, showWarnings = FALSE)
# ------------------------------
# 4. Download Function
# ------------------------------
download_one <- function(url, id) {
destfile <- file.path(save_dir, paste0("govfile_", id, ".pdf"))
tryCatch({
download.file(url, destfile, mode = "wb")
Sys.sleep(runif(1, 1.5, 3))   # cooldown
message("Downloaded: ", destfile)
}, error = function(e) {
message("FAILED: ", url)
})
}
# ------------------------------
# 5. Download the 10 PDFs
# ------------------------------
walk2(govfiles$pdf_url, govfiles$index, download_one)
## Scraping “Foreign Relations Committee” documents from GovInfo
library(httr)
library(jsonlite)
library(dplyr)
library(purrr)
library(magrittr)
api_key <- "4Extd9hhomZZ85LFaZec5xuYWuQSlxIzbTH8IjFc"
# ------------------------------
# 1. API SEARCH REQUEST
# ------------------------------
body <- list(
query = 'Trade AND "Foreign Relations Committee"',
pageSize = 10,          # <-- ONLY 10 MOST RECENT
offsetMark = "*"
)
resp <- POST(
url   = "https://api.govinfo.gov/search",
query = list(api_key = api_key),
encode = "json",
body = body
)
res <- jsonlite::fromJSON(content(resp, "text", encoding = "UTF-8"))
govfiles <- res$results
# ------------------------------
# 2. Construct PDF Links
# ------------------------------
govfiles$index <- seq_len(nrow(govfiles))
govfiles$pdf_url <- paste0(
"https://www.govinfo.gov/content/pkg/",
govfiles$packageId,
"/pdf/",
govfiles$packageId,
".pdf"
)
# ------------------------------
# 3. Prepare for Downloading
# ------------------------------
save_dir <- '/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5'
dir.create(save_dir, recursive = TRUE, showWarnings = FALSE)
# ------------------------------
# 4. Download Function
# ------------------------------
download_one <- function(url, id) {
destfile <- file.path(save_dir, paste0("govfile_", id, ".pdf"))
tryCatch({
download.file(url, destfile, mode = "wb")
Sys.sleep(runif(1, 1.5, 3))   # cooldown
message("Downloaded: ", destfile)
}, error = function(e) {
message("FAILED: ", url)
})
}
# ------------------------------
# 5. Download the 10 PDFs
# ------------------------------
walk2(govfiles$pdf_url, govfiles$index, download_one)
setwd("~/OneDrive - The University of Texas at Dallas/UTD/Fall2025/Data collection/assignment_5")
# quanteda_textanalytics01.R
# Analyze Biden-Xi summit tweets (improved preprocessing + LSA + hashtag/user networks)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(readr)
library(dplyr)
library(ggplot2)
library(stringr)
# Load tweet CSV (remote file in your example)
summit <- read_csv("https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv")
# Basic cleaning function for tweets
clean_tweet_text <- function(x) {
x %>%
str_replace_all("http[^\\s]+", "") %>%         # remove URLs
str_replace_all("RT\\s+@\\w+:", "") %>%        # remove retweet prefix
str_replace_all("@\\w+", "") %>%               # remove @mentions (keep separate analysis later)
str_replace_all("#", "") %>%                   # remove hash symbol (or keep depending on analysis)
str_replace_all("[[:punct:]]+", " ") %>%
str_squish() %>%
tolower()
}
summit <- summit %>%
mutate(text_clean = clean_tweet_text(text),
text_nohash = str_replace_all(text, "#", ""))  # keep original if you want hashes
# Make tokens / dfm
toks <- tokens(summit$text_clean, remove_punct = TRUE) %>%
tokens_remove(pattern = stopwords("english")) %>%
tokens_remove(pattern = c("u", "rt"))            # example domain stopwords
dfm_sum <- dfm(toks) %>%
dfm_trim(min_termfreq = 5)                       # drop very rare terms for stability
# Top features
top_terms <- topfeatures(dfm_sum, 30)
print(top_terms)
# LSA (on weighted dfm, e.g., tfidf)
dfm_tfidf <- dfm_tfidf(dfm_sum)
lsa_model <- textmodel_lsa(dfm_tfidf, nd = 4)
summary(lsa_model)
# Inspect documents coordinates (first 2 dimensions)
doc_coords <- as.data.frame(lsa_model$docs) %>%
tibble::rownames_to_column("doc_id")
head(doc_coords)
# If there's metadata (user, time), we can color or group documents
if("user" %in% names(summit)) {
doc_coords <- doc_coords %>%
left_join(summit %>% mutate(doc_id = row_number()) %>% select(doc_id, user), by = "doc_id")
}
# Hashtag analysis (extract from original text)
hashtags <- tokens(summit$text, pattern = "#", remove_punct = FALSE) %>%
tokens_select("#*", selection = "keep") %>%
dfm()
top_hashtags <- topfeatures(hashtags, 30)
print(top_hashtags)
# Hashtag network (co-occurrence)
tag_fcm <- fcm(hashtags)
top_tagnames <- names(top_hashtags)[1:30]
tag_fcm_sel <- fcm_select(tag_fcm, pattern = top_tagnames)
textplot_network(tag_fcm_sel, min_freq = 20, edge_alpha = 0.8, edge_size = 1)
# Mention (user) analysis
user_tokens <- tokens(summit$text, pattern = "@", remove_punct = FALSE) %>%
tokens_select("@*", selection = "keep")
user_dfm <- dfm(user_tokens)
top_users <- topfeatures(user_dfm, 30)
user_fcm <- fcm(user_dfm) %>% fcm_select(names(top_users))
textplot_network(user_fcm, min_freq = 10, edge_alpha = 0.8)
# Save artifacts
write_rds(dfm_sum, '/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_6/dfm_summit.rds')
write_csv(doc_coords, '/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_6/lsa_document_coords.csv')
##############################################################
# quanteda_textanalytics01.R
# Text Analytics: Biden–Xi Summit Tweets
# Improved preprocessing, LSA, hashtag and user networks
##############################################################
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(readr)
library(dplyr)
library(ggplot2)
library(stringr)
library(tibble)
# ------------------------------------------------------------
# 1. Load the data
# ------------------------------------------------------------
summit <- read_csv(
"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv"
)
# ------------------------------------------------------------
# 2. Improved preprocessing
# ------------------------------------------------------------
clean_tweet_text <- function(x) {
x %>%
str_replace_all("http[^\\s]+", "") %>%         # URLs
str_replace_all("RT\\s+@\\w+:", "") %>%        # RT flag
str_replace_all("@\\w+", "") %>%               # remove @mentions (kept separately later)
str_replace_all("#", "") %>%                   # remove hash marks
str_replace_all("[[:punct:]]+", " ") %>%       # punctuation
str_squish() %>%                               # collapse whitespace
tolower()
}
summit <- summit %>%
mutate(
text_clean = clean_tweet_text(text),
text_nohash = str_replace_all(text, "#", "")
)
# ------------------------------------------------------------
# 3. Tokenization and dfm creation
# ------------------------------------------------------------
toks <- tokens(
summit$text_clean,
remove_punct = TRUE,
remove_numbers = TRUE
) %>%
tokens_remove(stopwords("english")) %>%
tokens_remove(c("rt", "amp")) # common Twitter junk tokens
dfm_sum <- dfm(toks) %>%
dfm_trim(min_termfreq = 5)
# Top terms
top_terms <- topfeatures(dfm_sum, 30)
print(top_terms)
# ------------------------------------------------------------
# 4. LSA on TF–IDF weighted dfm
# ------------------------------------------------------------
dfm_tfidf <- dfm_tfidf(dfm_sum)
lsa_model <- textmodel_lsa(dfm_tfidf, nd = 4)
summary(lsa_model)
# Document coordinates
doc_coords <- as.data.frame(lsa_model$docs) %>%
rownames_to_column("doc_id") %>%
mutate(doc_id = as.numeric(doc_id))
# Add metadata (optional)
if ("user" %in% names(summit)) {
doc_coords <- doc_coords %>%
left_join(
summit %>% mutate(doc_id = row_number()) %>% select(doc_id, user),
by = "doc_id"
)
}
# ------------------------------------------------------------
# 5. Hashtag analysis (network)
# ------------------------------------------------------------
hashtags <- tokens(
summit$text,
remove_punct = FALSE
) %>%
tokens_select("#*", selection = "keep") %>%
dfm()
top_hashtags <- topfeatures(hashtags, 30)
print(top_hashtags)
tag_fcm <- fcm(hashtags)
top_tagnames <- names(top_hashtags)
tag_fcm_sel <- fcm_select(tag_fcm, pattern = top_tagnames)
textplot_network(
tag_fcm_sel,
min_freq = 20,
edge_alpha = 0.8,
edge_size = 1
)
# ------------------------------------------------------------
# 6. User mention network (@users)
# ------------------------------------------------------------
user_tokens <- tokens(
summit$text,
remove_punct = FALSE
) %>%
tokens_select("@*", selection = "keep")
user_dfm <- dfm(user_tokens)
top_users <- topfeatures(user_dfm, 30)
user_fcm <- fcm(user_dfm)
user_fcm_sel <- fcm_select(user_fcm, names(top_users))
textplot_network(
user_fcm_sel,
min_freq = 10,
edge_alpha = 0.8
)
# ------------------------------------------------------------
# 7. Save output files
# ------------------------------------------------------------
save_path <- "/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_6/"
write_rds(dfm_sum, paste0(save_path, "dfm_summit.rds"))
write_csv(doc_coords, paste0(save_path, "lsa_document_coords.csv"))
message("Files saved successfully to:")
message(save_path)
##############################################################
# Analyzing 3 different eras
# -------------------------
# 1. Define historical eras
# -------------------------
inaug <- data_corpus_inaugural
inaug$Era <- case_when(
inaug$Year <= 1865 ~ "Early Republic",
inaug$Year > 1865 & inaug$Year <= 1945 ~ "Industrial/Progressive",
inaug$Year > 1945 & inaug$Year <= 1991 ~ "Cold War",
inaug$Year > 1991 ~ "Modern"
)
# -------------------------
# 2. Construct DFM by Era
# -------------------------
dfm_era <- tokens(inaug, remove_punct = TRUE) %>%
tokens_remove(stopwords("english")) %>%
dfm() %>%
dfm_group(groups = inaug$Era) %>%
dfm_trim(min_termfreq = 10)
# -------------------------
# 3. Compare eras using keyness
#    Example: Modern vs. Early Republic
# -------------------------
key_era <- textstat_keyness(dfm_era, target = "Modern")
library(quanteda.textstats)
# Analyzing 3 different eras
# -------------------------
# 1. Define historical eras
# -------------------------
inaug <- data_corpus_inaugural
inaug$Era <- case_when(
inaug$Year <= 1865 ~ "Early Republic",
inaug$Year > 1865 & inaug$Year <= 1945 ~ "Industrial/Progressive",
inaug$Year > 1945 & inaug$Year <= 1991 ~ "Cold War",
inaug$Year > 1991 ~ "Modern"
)
# -------------------------
# 2. Construct DFM by Era
# -------------------------
dfm_era <- tokens(inaug, remove_punct = TRUE) %>%
tokens_remove(stopwords("english")) %>%
dfm() %>%
dfm_group(groups = inaug$Era) %>%
dfm_trim(min_termfreq = 10)
# -------------------------
# 3. Compare eras using keyness
#    Example: Modern vs. Early Republic
# -------------------------
key_era <- textstat_keyness(dfm_era, target = "Modern")
textplot_keyness(key_era, n = 20)
# -------------------------
# 4. Compare frequency of ideological terms over eras
# -------------------------
focus_terms <- c("freedom", "war", "economy", "people", "nation")
dfm_rel <- dfm_weight(dfm_era, scheme = "prop") * 100  # percentages
freq_rel <- textstat_frequency(dfm_rel, groups = docvars(dfm_rel, "groups"))
# Analyzing 3 different eras
# -------------------------
# 1. Define historical eras
# -------------------------
inaug <- data_corpus_inaugural
inaug$Era <- case_when(
inaug$Year <= 1865 ~ "Early Republic",
inaug$Year > 1865 & inaug$Year <= 1945 ~ "Industrial/Progressive",
inaug$Year > 1945 & inaug$Year <= 1991 ~ "Cold War",
inaug$Year > 1991 ~ "Modern"
)
# -------------------------
# 2. Construct DFM by Era
# -------------------------
corp_era <- corpus(inaug) %>% corpus_group(groups = "Era")
# Analyzing 3 different eras
# -------------------------
# 1. Define historical eras
# -------------------------
inaug <- data_corpus_inaugural
inaug$Era <- case_when(
inaug$Year <= 1865 ~ "Early Republic",
inaug$Year > 1865 & inaug$Year <= 1945 ~ "Industrial/Progressive",
inaug$Year > 1945 & inaug$Year <= 1991 ~ "Cold War",
inaug$Year > 1991 ~ "Modern"
)
# -------------------------
# 2. Construct DFM by Era
# -------------------------
dfm_era <- tokens(inaug, remove_punct = TRUE) %>%
tokens_remove(stopwords("english")) %>%
dfm() %>%
dfm_group(groups = inaug$Era) %>%
dfm_trim(min_termfreq = 10)
# -------------------------
# 3. Compare eras using keyness
#    Example: Modern vs. Early Republic
# -------------------------
key_era <- textstat_keyness(dfm_era, target = "Modern")
textplot_keyness(key_era, n = 20)
# -------------------------
# 4. Compare frequency of ideological terms over eras
# -------------------------
focus_terms <- c("freedom", "war", "economy", "people", "nation")
dfm_rel <- dfm_weight(dfm_era, scheme = "prop") * 100
# Add Era docvar
docvars(dfm_rel, "Era") <- docnames(dfm_rel)
freq_rel <- textstat_frequency(dfm_rel, groups = docvars(dfm_rel, "Era"))
freq_focus <- subset(freq_rel, feature %in% focus_terms)
freq_focus <- subset(freq_rel, feature %in% focus_terms)
ggplot(freq_focus, aes(x = group, y = frequency, color = feature)) +
geom_point(size = 3) +
geom_line(aes(group = feature), linewidth = 1) +
theme_minimal() +
labs(
title = "Change in Rhetorical Themes Across Historical Eras",
x = "Era",
y = "Relative Frequency (%)",
color = "Term"
)
# -------------------------
# 5. Top words in each era (comparison wordcloud)
# -------------------------
textplot_wordcloud(dfm_era, comparison = TRUE, max_words = 100)
# -------------------------
# 6. Measure stylistic / ideological shift using Wordfish
# -------------------------
wf <- textmodel_wordfish(dfm(tokens(inaug)), dir = c(1, 10))
wf_df <- data.frame(
Year = inaug$Year,
Era  = inaug$Era,
Position = wf$docs
)
ggplot(wf_df, aes(x = Year, y = Position, color = Era)) +
geom_line(size = 1.1) +
geom_point(size = 2) +
theme_minimal() +
labs(
title = "Wordfish Ideological/Rhetorical Position Over Time",
y = "Estimated Wordfish Position"
)
# Analyzing 3 different eras
# -------------------------
# 1. Define historical eras
# -------------------------
inaug <- data_corpus_inaugural
inaug$Era <- case_when(
inaug$Year <= 1865 ~ "Early Republic",
inaug$Year > 1865 & inaug$Year <= 1945 ~ "Industrial/Progressive",
inaug$Year > 1945 & inaug$Year <= 1991 ~ "Cold War",
inaug$Year > 1991 ~ "Modern"
)
# -------------------------
# 2. Construct DFM by Era
# -------------------------
dfm_era <- tokens(inaug, remove_punct = TRUE) %>%
tokens_remove(stopwords("english")) %>%
dfm() %>%
dfm_group(groups = inaug$Era) %>%
dfm_trim(min_termfreq = 10)
# -------------------------
# 3. Compare eras using keyness
#    Example: Modern vs. Early Republic
# -------------------------
key_era <- textstat_keyness(dfm_era, target = "Modern")
textplot_keyness(key_era, n = 20)
# -------------------------
# 4. Compare frequency of ideological terms over eras
# -------------------------
focus_terms <- c("freedom", "war", "economy", "people", "nation")
dfm_rel <- dfm_weight(dfm_era, scheme = "prop") * 100
# Add Era docvar
docvars(dfm_rel, "Era") <- docnames(dfm_rel)
freq_rel <- textstat_frequency(dfm_rel, groups = docvars(dfm_rel, "Era"))
freq_focus <- subset(freq_rel, feature %in% focus_terms)
freq_focus <- subset(freq_rel, feature %in% focus_terms)
ggplot(freq_focus, aes(x = group, y = frequency, color = feature)) +
geom_point(size = 3) +
geom_line(aes(group = feature), linewidth = 1) +
theme_minimal() +
labs(
title = "Change in Rhetorical Themes Across Historical Eras",
x = "Era",
y = "Relative Frequency (%)",
color = "Term"
)
# -------------------------
# 5. Top words in each era (comparison wordcloud)
# -------------------------
textplot_wordcloud(dfm_era, comparison = TRUE, max_words = 100)
# -------------------------
# 6. Measure stylistic / ideological shift using Wordfish
# -------------------------
wf <- textmodel_wordfish(dfm(tokens(inaug)), dir = c(1, 10))
wf_df <- data.frame(
Year = inaug$Year,
Era  = inaug$Era,
Position = wf$docs
)
ggplot(wf_df, aes(x = Year, y = Position, color = Era)) +
geom_line(size = 1.1) +
geom_point(size = 2) +
theme_minimal() +
labs(
title = "Wordfish Ideological/Rhetorical Position Over Time",
y = "Estimated Wordfish Position"
)
