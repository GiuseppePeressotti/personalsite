---
title: "Assignment 5"

---

```{r}
## Scraping “Foreign Relations Committee” documents from GovInfo


library(httr)
library(jsonlite)
library(dplyr)
library(purrr)
library(magrittr)

api_key <- "4Extd9hhomZZ85LFaZec5xuYWuQSlxIzbTH8IjFc"

# ------------------------------
# 1. API SEARCH REQUEST
# ------------------------------

body <- list(
  query = 'Trade AND "Foreign Relations Committee"',
  pageSize = 10,         
  offsetMark = "*"
)

resp <- POST(
  url   = "https://api.govinfo.gov/search",
  query = list(api_key = api_key),
  encode = "json",
  body = body
)

res <- jsonlite::fromJSON(content(resp, "text", encoding = "UTF-8"))

govfiles <- res$results

# ------------------------------
# 2. Construct PDF Links
# ------------------------------

govfiles$index <- seq_len(nrow(govfiles))
govfiles$pdf_url <- paste0(
  "https://www.govinfo.gov/content/pkg/",
  govfiles$packageId,
  "/pdf/",
  govfiles$packageId,
  ".pdf"
)

# ------------------------------
# 3. Prepare for Downloading
# ------------------------------

save_dir <- '/Users/bepi36/Library/CloudStorage/OneDrive-TheUniversityofTexasatDallas/UTD/Fall2025/Data collection/assignment_5'
dir.create(save_dir, recursive = TRUE, showWarnings = FALSE)

# ------------------------------
# 4. Download Function
# ------------------------------

download_one <- function(url, id) {
  
  destfile <- file.path(save_dir, paste0("govfile_", id, ".pdf"))
  
  tryCatch({
    download.file(url, destfile, mode = "wb")
    Sys.sleep(runif(1, 1.5, 3))   # cooldown
    message("Downloaded: ", destfile)
  }, error = function(e) {
    message("FAILED: ", url)
  })
}

# ------------------------------
# 5. Download the 10 PDFs
# ------------------------------

walk2(govfiles$pdf_url, govfiles$index, download_one)
```


# Report

The scraping process using the GovInfo API worked well overall. After implementing a few modifications to the original script, which comprised mainly refining the POST query structure, adjusting the search parameters, and constructing reliable PDF download links, the workflow successfully retrieved and downloaded the desired government documents related to the Foreign Relations Committee. Scraping functioned smoothly once the query had been properly structured.

However, several practical issues emerged during testing. First, the quality and usefulness of the results depend heavily on the chosen search terms (playing around with different number of documents). Minor variations in phrasing or keyword placement can return a different set of documents, some of which may be irrelevant, incomplete, or even entirely void. Additionally, depending on the size and number of files requested, the downloading process may take longer, and the availability of PDF versions is not guaranteed for all entries.  

The overall scraped corpus is still valuable. While the PDFs tend to be noisy—containing artifacts from OCR processing, embedded images, or poorly structured text—they nevertheless offer a workable dataset for text analysis and natural language processing. With appropriate preprocessing, they can be transformed into a rich source for extracting themes, patterns, or policy-relevant language.

Refining the search keywords, imposing more precise constraints (such as specific committees, date ranges, or document types), and filtering results programmatically before downloading can increase the quality and relevance of the final dataset.